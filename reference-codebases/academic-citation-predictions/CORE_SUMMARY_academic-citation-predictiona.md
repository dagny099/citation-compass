# Academic Citation Predictions - Core Functionality Summary

## Project Overview
**Academic citation network analysis system** using Semantic Scholar API for data ingestion and Neo4j for graph storage/analysis. While designed for PyTorch-based link prediction models, the current implementation focuses on data collection, transformation, and graph analytics. Machine learning capabilities are planned but not yet implemented.

## Data Flow Architecture

```
Semantic Scholar API ‚Üí Data Ingestion ‚Üí ETL Processing ‚Üí Graph Storage ‚Üí Analysis
                                    ‚Üì
                               Knowledge Graph
                            (Papers, Authors, Venues)
                                    ‚Üì
                              Neo4j Analytics
                           (Citation patterns, trends)
```

**Note**: PyKEEN and PyTorch Geometric integrations are planned but not currently implemented.

## Core Scripts & Functions

### 1. `helper_func.py` - API & Utility Functions ‚úÖ **EXISTS**
**Purpose**: Core utilities for Semantic Scholar API interaction and data processing

#### Key Functions:
- **`paginate_api_requests(base_url, params, ...)`** (lines 63-155)
  - Handles paginated API requests with rate limiting
  - Generic function for any Semantic Scholar endpoint
  - Returns generator of individual items
  - **Data Flow**: API ‚Üí Paginated Results ‚Üí Individual Records

- **`retrieve_citations_by_id(paper_id)`** (lines 157-172)  
  - Gets list of papers citing a given paper
  - Returns list of citing paper IDs
  - **Data Flow**: Paper ID ‚Üí Citations Endpoint ‚Üí List of Citing Paper IDs

- **`batch_paper_details(paperIds)`** (lines 174-192)
  - Bulk retrieval of paper metadata for multiple IDs
  - Uses POST batch endpoint for efficiency
  - **Data Flow**: List of Paper IDs ‚Üí Batch API ‚Üí Detailed Paper Records

- **`build_triples(details_list_dicts, subject_key, relation, predicate_key)`** (lines 16-32)
  - Converts structured data to RDF triples format
  - **Data Flow**: Raw Data Records ‚Üí Triple Format (Subject, Predicate, Object)

- **`search_endpoint_nopaginate(endpoint, query)`** (lines 34-61)
  - Simple search without pagination
  - **Data Flow**: Search Query ‚Üí API Search ‚Üí Results JSON

### 2. `etl_papers_authors.py` - Data Transformation Pipeline ‚úÖ **EXISTS**
**Purpose**: Transforms raw API data into normalized graph entities and relationships

#### Key Functions:
- **`transform_papers(data)`** (lines 70-86)
  - Extracts paper nodes with metadata
  - **Data Flow**: Raw API Data ‚Üí Paper Entities (paperId, title, citations, etc.)

- **`transform_authors(data)`** (lines 34-66)
  - Extracts author nodes and co-authorship relationships  
  - **Data Flow**: Raw API Data ‚Üí (Author Entities, Co-authorship Relations)

- **`transform_fields_of_study(data)`** (lines 6-31)
  - Extracts research fields and paper-field relationships
  - **Data Flow**: Raw API Data ‚Üí (Field Entities, Paper-Field Relations)

- **`transform_venues(data)`** (lines 89-108)
  - Extracts publication venues and paper-venue relationships
  - **Data Flow**: Raw API Data ‚Üí (Venue Entities, Paper-Venue Relations)

- **`transform_years(data)`** (lines 111-138)
  - Extracts publication years and paper-year relationships
  - **Data Flow**: Raw API Data ‚Üí (Year Entities, Paper-Year Relations)

### 3. `neo4j_insights.py` - Graph Analysis & Querying ‚úÖ **EXISTS**
**Purpose**: Analytics and insights from the knowledge graph stored in Neo4j

#### Key Functions:
- **`run_query(cypher_query, parameters)`** (lines 25-39)
  - Executes Cypher queries and returns pandas DataFrames
  - **Data Flow**: Cypher Query ‚Üí Neo4j Database ‚Üí DataFrame Results

- **`get_top_authors(limit)`** (lines 45-58)
  - Finds authors by total citation count
  - **Data Flow**: Neo4j Graph ‚Üí Author Citation Aggregation ‚Üí Ranked List

- **`get_top_venues(limit)`** (lines 61-74)
  - Finds venues by average citation count per paper
  - **Data Flow**: Neo4j Graph ‚Üí Venue Citation Analysis ‚Üí Ranked List

- **`get_coauthors(author_name, limit)`** (lines 77-93)
  - Finds frequent collaborators for a given author
  - **Data Flow**: Author Name ‚Üí Co-authorship Network ‚Üí Collaboration Rankings

- **`get_citation_histogram()`** (lines 96-106)
  - Retrieves citation distribution data
  - **Data Flow**: Neo4j Graph ‚Üí Citation Counts ‚Üí Distribution Data

- **`get_yearly_author_citations()`** (lines 109-121)
  - Gets temporal citation patterns by author
  - **Data Flow**: Neo4j Graph ‚Üí Time-Series Citation Data ‚Üí Author Trends

- **`get_rising_stars(min_years, top_n)`** (lines 124-148)
  - Identifies authors with increasing citation trends using linear regression
  - **Data Flow**: Yearly Citations ‚Üí Linear Regression Analysis ‚Üí Growth Rankings

### 4. `1-Data-Ingestion.ipynb` - Primary Data Collection Workflow ‚úÖ **EXISTS**
**Purpose**: Interactive data collection from Semantic Scholar API

#### Core Workflow:
1. **Search Phase**: Query API for seed paper by title
2. **Expansion Phase**: Recursively collect citing papers 
3. **Enrichment Phase**: Gather author, venue, field metadata
4. **Storage Phase**: Save raw data as JSON/CSV
5. **Graph Construction**: Convert to RDF triples and PyTorch Geometric objects

#### Key Data Structures Created:
- Citation networks (paper ‚Üí citing papers)
- Co-authorship networks (authors ‚Üí shared papers)  
- Knowledge graph triples (subject, predicate, object)

### 5. `2- Enrich Data-.ipynb` - Data Processing & Transformation ‚úÖ **EXISTS**
**Purpose**: Clean and structure raw data for graph database import

#### Core Workflow:
1. **Load Raw Data**: Read collected JSON data (9,212 papers) ‚úÖ
2. **Apply Transformations**: Use ETL functions to create normalized entities ‚úÖ
3. **Generate Relationships**: Create relationship tables for Neo4j import ‚úÖ
4. **Network Analysis**: Build co-authorship networks with NetworkX ‚úÖ
5. **Export Structured Data**: Save as CSV files for database import ‚úÖ

### 6. Supporting Files ‚úÖ **ALL EXIST**

#### `neo4j_full_import_script.cypher` ‚úÖ
- Complete Neo4j database schema and import script
- Creates constraints for data integrity
- Imports all entity types and relationships

#### `database-schema.txt` ‚úÖ 
- Documents Neo4j graph schema
- Node types: Paper, Author, PubVenue, PubYear, Field
- Relationship types: PUBLISHED_IN, CO_AUTHORED, IS_ABOUT, etc.

#### `src/data_ingestion/semantic_scholar_api.py` ‚úÖ **NEW - ENHANCED VERSION**
- Consolidated API client with proper error handling
- Class-based architecture with configuration management
- Enhanced citation network expansion capabilities

## Unique Functionality & Design Patterns

### 1. **Recursive Citation Network Expansion** ‚úÖ **IMPLEMENTED**
- `expand_nodes()` function in notebooks ‚úÖ
- `expand_citation_network()` method in `SemanticScholarAPI` class ‚úÖ
- Automatically discovers and expands citation networks
- **Unique Feature**: Self-expanding knowledge graph based on citation relationships

### 2. **Multi-Modal Data Integration** ‚úÖ **IMPLEMENTED**
- Combines bibliometric data (citations, references) ‚úÖ
- Author collaboration networks ‚úÖ
- Temporal publication patterns ‚úÖ
- Research field classifications ‚úÖ

### 3. **Hybrid Storage Strategy** ‚úÖ **PARTIALLY IMPLEMENTED**
- Raw data in JSON files ‚úÖ
- Processed data in CSV format ‚úÖ
- Graph data in Neo4j ‚úÖ
- PyTorch Geometric objects for ML ‚ùå **PLANNED BUT NOT IMPLEMENTED**

### 4. **Scalable API Interaction** ‚úÖ **FULLY IMPLEMENTED**
- Generic pagination handling ‚úÖ
- Rate limiting and error recovery ‚úÖ
- Batch processing for efficiency ‚úÖ

### 5. **Advanced Graph Analytics** ‚úÖ **IMPLEMENTED**
- Rising star detection using linear regression ‚úÖ
- Centrality analysis for author importance ‚úÖ (in notebooks)
- Temporal trend analysis ‚úÖ

## Data Pipeline Summary

```
1. INGESTION: Semantic Scholar API ‚Üí Raw JSON Data ‚úÖ
2. EXTRACTION: Raw Data ‚Üí Pandas DataFrames ‚úÖ 
3. TRANSFORMATION: DataFrames ‚Üí Graph Entities + Relationships ‚úÖ
4. LOADING: Structured Data ‚Üí Neo4j Graph Database ‚úÖ
5. ANALYSIS: Neo4j ‚Üí Insights + Visualizations ‚úÖ
6. MODELING: Graph Data ‚Üí PyTorch Geometric ‚Üí Link Predictions ‚ùå PLANNED
```

## Key Dependencies
- **Data Sources**: Semantic Scholar API ‚úÖ
- **Storage**: Neo4j graph database, JSON/CSV files ‚úÖ
- **Processing**: pandas ‚úÖ, NetworkX ‚úÖ, PyTorch Geometric ‚ùå **PLANNED**
- **ML Framework**: PyKEEN ‚ùå **PLANNED** (listed in pyproject.toml but not used)
- **Analysis**: scipy for statistical analysis ‚úÖ
- **Graph Database**: RDFLib ‚úÖ for triple storage

## Implementation Status Summary

### ‚úÖ **FULLY IMPLEMENTED**
- Data ingestion pipeline with Semantic Scholar API
- ETL transformation functions for graph entities
- Neo4j graph database integration
- Advanced analytics (rising stars, centrality, trends)
- Hybrid storage strategy (JSON, CSV, Neo4j)
- Rate limiting and error handling
- Citation network expansion algorithms

### ‚ùå **PLANNED BUT NOT IMPLEMENTED**
- PyTorch Geometric graph objects
- PyKEEN knowledge graph embeddings  
- Link prediction models
- Machine learning pipeline

### üìÇ **DATA ASSETS CREATED**
- 9,212 academic papers collected
- Citation network with 318+ citing relationships
- Author collaboration networks
- Transformed CSV files ready for Neo4j import
- RDF triples for semantic web applications

## Comparison Points for Other Codebases
When comparing with other projects, focus on:

1. **API Integration Strategy** ‚úÖ - Robust pagination and rate limiting
2. **Graph Construction Approach** ‚úÖ - Multi-entity modeling with relationships
3. **ML Model Architecture** ‚ùå - **NOT YET IMPLEMENTED**
4. **Scalability Patterns** ‚úÖ - Batch processing and recursive expansion
5. **Analytics Capabilities** ‚úÖ - Unique rising star detection and trend analysis
6. **Storage Architecture** ‚úÖ - Multi-format storage strategy